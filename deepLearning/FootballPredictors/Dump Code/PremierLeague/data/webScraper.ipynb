{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import os, time, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_duplicates(df):\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique(): \n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + '_' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    df.columns = cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_result(score):\n",
    "    try:\n",
    "        # Replace various types of dashes and special characters with a standard hyphen\n",
    "        score = score.replace('–', '-').replace('—', '-').replace('−', '-')\n",
    "        # Split the score into home and away scores\n",
    "        home_score, away_score = map(int, score.split('-'))\n",
    "        if home_score > away_score:\n",
    "            return 'H'\n",
    "        elif home_score < away_score:\n",
    "            return 'A'\n",
    "        else:\n",
    "            return 'D'\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing score '{score}': {e}\")\n",
    "        return 'Invalid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_process_data(season):\n",
    "    # Determine the URLs based on the season\n",
    "    if season == '2024/2025':\n",
    "        url1 = \"https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures\"\n",
    "        url2 = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "    else:\n",
    "        # Modify URL for past seasons\n",
    "        season_formatted = season.replace('/', '-')\n",
    "        url1 = f\"https://fbref.com/en/comps/9/{season_formatted}/schedule/{season_formatted}-Premier-League-Scores-and-Fixtures\"\n",
    "        url2 = f\"https://fbref.com/en/comps/9/{season_formatted}/{season_formatted}-Premier-League-Stats\"\n",
    "\n",
    "    # Fetch and process the scores and fixtures\n",
    "    response = requests.get(url1)\n",
    "    html_content = StringIO(response.text)\n",
    "    tables = pd.read_html(html_content)\n",
    "\n",
    "    if len(tables) >= 1:\n",
    "        table1 = tables[0]\n",
    "        table1 = table1.drop(columns=['Day', 'Date', 'Time', 'Attendance', 'Venue', 'Referee', 'Match Report', 'Notes'])\n",
    "        table1 = table1.dropna()\n",
    "        table1 = table1.dropna(axis=1)\n",
    "\n",
    "        if 'Score' in table1.columns:\n",
    "            table1['Result'] = table1['Score'].apply(determine_result)\n",
    "            score_index = table1.columns.get_loc('Score')\n",
    "            table1.insert(score_index + 1, 'Result', table1.pop('Result'))\n",
    "            table1 = table1.drop(columns=['Score'])\n",
    "        table1['Season'] = season\n",
    "    else:\n",
    "        print(f\"There are more than one table on the Scores Page for season {season}.\")\n",
    "        return None, None\n",
    "\n",
    "    # Fetch and process the stats\n",
    "    response = requests.get(url2)\n",
    "    html_content = StringIO(response.text)\n",
    "    tables = pd.read_html(html_content)\n",
    "\n",
    "    indices_to_keep = {0, 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22}\n",
    "    filtered_tables = [table for i, table in enumerate(tables) if i in indices_to_keep]\n",
    "\n",
    "    for i, table in enumerate(filtered_tables):\n",
    "        if i != 0:\n",
    "            table.columns = table.columns.droplevel(0)\n",
    "\n",
    "    columns_to_delete_table_0 = ['Last 5', 'Attendance', 'Top Team Scorer', 'Goalkeeper', 'Notes']\n",
    "    filtered_tables[0] = filtered_tables[0].drop(columns=[col for col in columns_to_delete_table_0 if col in filtered_tables[0].columns], errors='ignore')\n",
    "    table_1_Columns = filtered_tables[1].columns\n",
    "    new_table_1_Columns = ['Rk', 'Squad'] + [f'H{col}' for col in table_1_Columns[2:14]] + [f'A{col}' for col in table_1_Columns[14:]]\n",
    "    filtered_tables[1].columns = new_table_1_Columns\n",
    "    if filtered_tables[2].columns[22] == 'Gls':\n",
    "        filtered_tables[2].drop(filtered_tables[2].columns[22], axis=1, inplace=True)\n",
    "\n",
    "    combined_table = pd.DataFrame()\n",
    "\n",
    "    for table in filtered_tables:\n",
    "        columns_to_delete = ['90s', 'Starts', '# Pl', 'Min']\n",
    "        table = table.drop(columns=[col for col in columns_to_delete if col in table.columns], errors='ignore')\n",
    "        \n",
    "        if 'Squad' not in table.columns:\n",
    "            raise ValueError(\"'Squad' column is missing from one of the tables.\")\n",
    "        \n",
    "        if combined_table.empty:\n",
    "            combined_table = table\n",
    "        else:\n",
    "            columns_to_add = [col for col in table.columns if col not in combined_table.columns]\n",
    "            if columns_to_add:\n",
    "                combined_table = combined_table.merge(table[['Squad'] + columns_to_add], on='Squad', how='left')\n",
    "\n",
    "    home_stats = combined_table.add_prefix(f'Home_')\n",
    "    away_stats = combined_table.add_prefix(f'Away_')\n",
    "\n",
    "    # Merge home and away stats with fixtures\n",
    "    table1 = table1.merge(home_stats, how='left', left_on='Home', right_on='Home_Squad')\n",
    "    table1 = table1.merge(away_stats, how='left', left_on='Away', right_on='Away_Squad')\n",
    "\n",
    "    # Drop unnecessary columns (like 'Home_Squad' and 'Away_Squad') after merging\n",
    "    table1 = table1.drop(columns=[f'Home_Squad', f'Away_Squad'])\n",
    "\n",
    "    return table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['2019/2020', '2020/2021', '2021/2022', '2022/2023', '2023/2024', '2024/2025']\n",
    "\n",
    "all_tables = []\n",
    "\n",
    "for season in seasons:\n",
    "    time.sleep(random.uniform(6, 10))\n",
    "    table = fetch_and_process_data(season)\n",
    "    if table is not None:\n",
    "        all_tables.append(table)\n",
    "\n",
    "# Concatenate all season data into one table\n",
    "final_table = pd.concat(all_tables, ignore_index=True)\n",
    "final_table = final_table.dropna(axis=1)\n",
    "final_table = rename_duplicates(final_table)\n",
    "final_table.to_csv('final44.csv', index=False)\n",
    "print('done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
